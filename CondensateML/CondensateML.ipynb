{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea84d55c-eca6-484d-b083-990b0dd136a9",
   "metadata": {},
   "source": [
    "## CondensateML\n",
    "A deep-learning pipeline for condensate phenotyping and image-based clustering from high-content screens\n",
    "\n",
    "### Features\n",
    "- ResNet18-based feature extraction (512D embeddings)\n",
    "- UMAP projection + anchor-based similarity scoring\n",
    "- ClusterProfiler-based GO enrichment integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c011e0-815a-42dd-a7a0-f5f64bb6c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import models\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import kornia.augmentation as K\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import cv2\n",
    "\n",
    "# ==== Inference: Embedding similarity vs RNF26/ZNF335 ====\n",
    "import os, re, numpy as np, pandas as pd, cv2\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import label, regionprops_table\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"   # forces sync; gives accurate stack traces\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # optional, helps determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ddb572-4273-4e19-86d7-ffaeb7eefdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Preprocessing & Labeling (Regression: RNF=-1, Neutral=0, ZNF=+1) ====\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage.io import imread\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.transform import resize\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "input_dir = \"path_to_training_data\"\n",
    "crop_size, resize_size, half_crop = 100, 224, 50\n",
    "\n",
    "use_channels = [\"GFP\"]   # which channels to include\n",
    "\n",
    "#Update channel mapping as needed:\n",
    "channel_map = {\n",
    "    \"DAPI\": \"_w1.TIF\",\n",
    "    \"GFP\": \"_w2.TIF\",\n",
    "    \"CellMask\": \"_w3.TIF\"\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Load images\n",
    "# -----------------------------\n",
    "stack, fov_filenames = [], []\n",
    "fov_filenames = sorted([fn.replace(channel_map[\"DAPI\"], \"\")\n",
    "                        for fn in os.listdir(input_dir) if fn.endswith(channel_map[\"DAPI\"])])\n",
    "\n",
    "for prefix in tqdm(fov_filenames, desc=\"Loading FOVs\", unit=\"FOV\"):\n",
    "    img_w1 = imread(os.path.join(input_dir, prefix + channel_map[\"DAPI\"]))\n",
    "    imgs = {\"DAPI\": img_w1}\n",
    "\n",
    "    for ch in use_channels:\n",
    "        fn = prefix + channel_map[ch]\n",
    "        if os.path.exists(os.path.join(input_dir, fn)):\n",
    "            imgs[ch] = imread(os.path.join(input_dir, fn))\n",
    "\n",
    "    if any(img.shape != img_w1.shape for img in imgs.values()):\n",
    "        continue\n",
    "    stack.append(imgs)\n",
    "\n",
    "print(f\"Loaded {len(stack)} FOVs\")\n",
    "\n",
    "# -----------------------------\n",
    "# Segment nuclei + crop patches\n",
    "# -----------------------------\n",
    "all_patches, output_rows = [], []\n",
    "for fov_idx, (imgs, fov_name) in tqdm(enumerate(zip(stack, fov_filenames)),\n",
    "                                      total=len(stack), desc=\"Cropping patches\", unit=\"FOV\"):\n",
    "    dapi = imgs[\"DAPI\"]\n",
    "    thresh = threshold_otsu(dapi)\n",
    "    labeled = label(dapi > thresh)\n",
    "    regions = regionprops(labeled, intensity_image=dapi)\n",
    "\n",
    "    for region in regions:\n",
    "        cy, cx = map(int, region.centroid)\n",
    "        y1, y2, x1, x2 = cy-half_crop, cy+half_crop, cx-half_crop, cx+half_crop\n",
    "        if y1 < 0 or y2 > dapi.shape[0] or x1 < 0 or x2 > dapi.shape[1]:\n",
    "            continue\n",
    "\n",
    "        patch_channels = []\n",
    "        for ch in use_channels:\n",
    "            patch = resize(imgs[ch][y1:y2, x1:x2],\n",
    "                           (resize_size, resize_size),\n",
    "                           preserve_range=True)\n",
    "            patch_channels.append(patch)\n",
    "\n",
    "        patch = np.stack(patch_channels, axis=0).astype(np.float32)\n",
    "        all_patches.append(patch)\n",
    "        output_rows.append({\"name\": fov_name})\n",
    "\n",
    "patches = np.stack(all_patches, axis=0)\n",
    "print(f\"Patch tensor shape: {patches.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Build metadata + assign labels\n",
    "# -----------------------------\n",
    "meta_df = pd.DataFrame(output_rows)\n",
    "np.save(\"resnet18_patches.npy\", patches)\n",
    "meta_df.to_csv(\"resnet18_patch_metadata.csv\", index=False)\n",
    "\n",
    "# Reload (safe practice)\n",
    "patches = np.load(\"resnet18_patches.npy\")\n",
    "meta_df = pd.read_csv(\"resnet18_patch_metadata.csv\")\n",
    "\n",
    "# Extract merge key\n",
    "meta_df[\"merge_key\"] = meta_df[\"name\"].apply(lambda x: re.search(r\"(AC\\d+_\\w\\d+)\", x).group(1))\n",
    "\n",
    "# Plate-to-gene mapping\n",
    "raw = pd.read_csv(\"names.csv\", skiprows=2)\n",
    "raw = raw.iloc[:, [0, 1, 2]]\n",
    "raw.columns = [\"Gene\", \"Plate\", \"Well\"]\n",
    "raw.dropna(subset=[\"Gene\", \"Plate\", \"Well\"], inplace=True)\n",
    "raw[\"merge_key\"] = raw[\"Plate\"].astype(str).str.strip() + \"_\" + raw[\"Well\"].astype(str).str.strip()\n",
    "\n",
    "meta_df = meta_df.merge(raw[[\"merge_key\", \"Gene\"]], on=\"merge_key\", how=\"left\")\n",
    "\n",
    "# Labels: RNF=-1, Neutral=0, ZNF=+1\n",
    "meta_df[\"label\"] = np.nan\n",
    "meta_df.loc[meta_df[\"merge_key\"].str.endswith(\"B02\"), \"label\"] = 0\n",
    "meta_df.loc[meta_df[\"Gene\"] == \"ZNF335\", \"label\"] = 1\n",
    "meta_df.loc[meta_df[\"Gene\"] == \"RNF26\", \"label\"] = -1\n",
    "\n",
    "# Keep labeled rows only\n",
    "valid_idx = meta_df.index[~meta_df[\"label\"].isna()]\n",
    "patches = patches[valid_idx]\n",
    "meta_df = meta_df.loc[valid_idx].reset_index(drop=True)\n",
    "\n",
    "# Clean\n",
    "meta_df[\"Gene\"] = meta_df[\"Gene\"].replace([\"\", \" \", \"nan\", \"NaN\"], np.nan).fillna(\"NT\")\n",
    "\n",
    "print(\"Positive genes in training:\", meta_df[meta_df.label == 1].Gene.unique())\n",
    "print(\"Negative genes in training:\", meta_df[meta_df.label == -1].Gene.unique())\n",
    "print(\"Neutral wells in training:\", meta_df[meta_df.label == 0].merge_key.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79150685-2c4f-4b10-a995-cb5fbafdc48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Training (Regression: RNF=-1, Neutral=0, ZNF=+1) ====\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch, torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np, random\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# -----------------------------\n",
    "# Reproducibility\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# Model (regression, 1 output, same as inference)\n",
    "# -----------------------------\n",
    "class ResNetRegression(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, freeze_until_layer=6):\n",
    "        super().__init__()\n",
    "        # Use pretrained ImageNet weights\n",
    "        base = resnet18(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "        # Freeze early layers\n",
    "        child_counter = 0\n",
    "        for child in base.children():\n",
    "            child_counter += 1\n",
    "            if child_counter < freeze_until_layer:  # freeze conv1..layer2\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        # Replace final FC\n",
    "        num_ftrs = base.fc.in_features\n",
    "        base.fc = nn.Identity()\n",
    "        self.base = base\n",
    "\n",
    "        # Head\n",
    "        self.fc1 = nn.Linear(num_ftrs, hidden_dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Initialize head\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity=\"relu\")\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity=\"linear\")\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "\n",
    "    def forward(self, x, return_embed=False):\n",
    "        if x.shape[1] < 3:  # ensure 3-channel input\n",
    "            pad = torch.zeros((x.shape[0], 3 - x.shape[1], x.shape[2], x.shape[3]),\n",
    "                              device=x.device, dtype=x.dtype)\n",
    "            x = torch.cat([x, pad], dim=1)\n",
    "        feats = self.base(x)\n",
    "        if return_embed:\n",
    "            return feats\n",
    "        x = self.fc1(feats)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = ResNetRegression(hidden_dim=64, freeze_until_layer=6).to(device)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset prep\n",
    "# -----------------------------\n",
    "labels = meta_df[\"label\"].values   # already -1,0,1\n",
    "X = torch.tensor((patches - patches.mean()) / patches.std(), dtype=torch.float32)   # z-score norm\n",
    "X = X.repeat(1, 3, 1, 1)  # (N,3,H,W)\n",
    "\n",
    "# Separate classes\n",
    "idx_rnf = np.where(labels == -1)[0]\n",
    "idx_neu = np.where(labels == 0)[0]\n",
    "idx_znf = np.where(labels == 1)[0]\n",
    "print(\"Before undersampling:\", len(idx_rnf), len(idx_neu), len(idx_znf))\n",
    "\n",
    "# Undersample Neutral (0)\n",
    "min_size = min(len(idx_rnf), len(idx_znf))\n",
    "idx_neu_down = resample(idx_neu, replace=False, n_samples=min_size, random_state=SEED)\n",
    "\n",
    "# Combine balanced\n",
    "balanced_idx = np.hstack([idx_rnf, idx_neu_down, idx_znf])\n",
    "np.random.shuffle(balanced_idx)\n",
    "\n",
    "X_bal = X[balanced_idx]\n",
    "y_bal = torch.tensor(labels[balanced_idx], dtype=torch.float32)\n",
    "print(\"After undersampling:\", (y_bal == -1).sum().item(),\n",
    "      (y_bal == 0).sum().item(), (y_bal == 1).sum().item())\n",
    "\n",
    "# Train/val split\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(y_bal)),\n",
    "    test_size=0.2,\n",
    "    stratify=y_bal.numpy(),\n",
    "    random_state=SEED\n",
    ")\n",
    "train_ds = TensorDataset(X_bal[train_idx], y_bal[train_idx])\n",
    "val_ds   = TensorDataset(X_bal[val_idx],   y_bal[val_idx])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Optimizer, Loss, Scheduler\n",
    "# -----------------------------\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-2)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop (with history logging)\n",
    "# -----------------------------\n",
    "train_history = []  # <-- store metrics here\n",
    "\n",
    "def train_with_early_stopping(max_epochs=100, patience=25):\n",
    "    best_val_loss, patience_counter = float(\"inf\"), 0\n",
    "    for epoch in range(max_epochs):\n",
    "        # --- Train ---\n",
    "        model.train(); train_losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device).unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward(); optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        train_loss = np.mean(train_losses)\n",
    "\n",
    "        # --- Validate ---\n",
    "        model.eval(); val_preds, val_truth, val_losses = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device).unsqueeze(1)\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                val_losses.append(loss.item())\n",
    "                val_preds.extend(preds.cpu().numpy().ravel())\n",
    "                val_truth.extend(yb.cpu().numpy().ravel())\n",
    "        val_loss = np.mean(val_losses)\n",
    "\n",
    "        # --- Metrics ---\n",
    "        val_truth = np.array(val_truth); val_preds = np.array(val_preds)\n",
    "        mae = mean_absolute_error(val_truth, val_preds)\n",
    "        corr = pearsonr(val_truth, val_preds)[0]\n",
    "        val_preds_class = np.rint(val_preds).astype(int)\n",
    "        acc = (val_preds_class == val_truth).mean()\n",
    "\n",
    "        auc = np.nan\n",
    "        mask = np.isin(val_truth, [-1,1])\n",
    "        if mask.sum() > 0:\n",
    "            truth_bin = (val_truth[mask] == 1).astype(int)\n",
    "            scores = (np.clip(val_preds[mask], -1, 1) + 1) / 2\n",
    "            auc = roc_auc_score(truth_bin, scores)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d}/{max_epochs} | TrainLoss={train_loss:.4f} | \"\n",
    "              f\"ValLoss={val_loss:.4f} | MAE={mae:.3f} | Corr={corr:.3f} | \"\n",
    "              f\"Acc={acc:.3f} | AUC={auc:.3f}\")\n",
    "\n",
    "        # --- Save to history ---\n",
    "        train_history.append({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": acc,\n",
    "            \"mae\": mae,\n",
    "            \"corr\": corr,\n",
    "            \"auc\": auc\n",
    "        })\n",
    "\n",
    "        # --- Early stopping ---\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss, patience_counter = val_loss, 0\n",
    "            torch.save(model.state_dict(), \"similarity_model_best.pt\")\n",
    "            print(f\"Saved best model (ValLoss={val_loss:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "        scheduler.step()\n",
    "\n",
    "train_with_early_stopping(max_epochs=100, patience=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef6f5a-9f30-40e9-89b3-57b75884a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Full Evaluation & Training Plots (6-panel + individual saves) ====\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt, torch\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, r2_score, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, roc_curve, roc_auc_score\n",
    ")\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "CKPT = \"similarity_model_best.pt\"\n",
    "custom_colors = [\"#f2704e\", \"#e13c3d\", \"#921d5c\", \"#651f56\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Collect predictions\n",
    "# -----------------------------\n",
    "y_true, y_pred = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device).unsqueeze(1)\n",
    "        preds = model(xb)\n",
    "        y_true.extend(yb.cpu().numpy().ravel())\n",
    "        y_pred.extend(preds.cpu().numpy().ravel())\n",
    "y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "r2   = r2_score(y_true, y_pred)\n",
    "corr = pearsonr(y_true, y_pred)[0] if len(y_true) > 1 else np.nan\n",
    "y_pred_class = np.rint(y_pred).astype(int)\n",
    "acc = accuracy_score(y_true, y_pred_class)\n",
    "cm  = confusion_matrix(y_true, y_pred_class, labels=[-1,0,1])\n",
    "\n",
    "mask_bin = np.isin(y_true, [-1,1])\n",
    "roc_auc, fpr, tpr = np.nan, None, None\n",
    "if mask_bin.sum() > 0:\n",
    "    y_true_bin = (y_true[mask_bin] == 1).astype(int)\n",
    "    y_score_bin = y_pred[mask_bin]\n",
    "    roc_auc = roc_auc_score(y_true_bin, y_score_bin)\n",
    "    fpr, tpr, _ = roc_curve(y_true_bin, y_score_bin)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"MAE={mae:.4f}, R²={r2:.4f}, Corr={corr:.4f}, Acc={acc:.4f}, AUC={roc_auc:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Training history\n",
    "# -----------------------------\n",
    "hist = None\n",
    "if \"train_history\" in globals() and len(train_history) > 0:\n",
    "    hist = pd.DataFrame(train_history)\n",
    "\n",
    "# -----------------------------\n",
    "# 6-panel summary\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 15))\n",
    "\n",
    "# 1. Scatter\n",
    "color_map = {-1: custom_colors[0], 0: custom_colors[1], 1: custom_colors[2]}\n",
    "axes[0,0].scatter(y_true, y_pred, c=[color_map[int(lbl)] for lbl in y_true],\n",
    "                  alpha=0.6, edgecolor=\"k\")\n",
    "for h in [-1,0,1]:\n",
    "    axes[0,0].axhline(h, color='grey', linestyle='--')\n",
    "    axes[0,0].axvline(h, color='grey', linestyle='--')\n",
    "axes[0,0].set_title(f\"True vs Predicted (Corr={corr:.2f})\")\n",
    "\n",
    "# 2. Histogram\n",
    "for i,(cls,label) in enumerate([(-1,\"RNF26 (-1)\"), (0,\"Neutral (0)\"), (1,\"ZNF335 (+1)\")]):\n",
    "    axes[0,1].hist(y_pred[y_true==cls], bins=20, alpha=0.8,\n",
    "                   label=label, color=custom_colors[i])\n",
    "axes[0,1].set_title(\"Prediction Distribution\"); axes[0,1].legend()\n",
    "\n",
    "# 3. ROC\n",
    "if fpr is not None:\n",
    "    axes[1,0].plot(fpr, tpr, lw=2, color=custom_colors[3])\n",
    "    axes[1,0].plot([0,1],[0,1],'--',color='grey')\n",
    "    axes[1,0].text(0.05, 0.95, f\"AUC = {roc_auc:.3f}\",\n",
    "                   transform=axes[1,0].transAxes, va=\"top\", ha=\"left\",\n",
    "                   bbox=dict(facecolor=\"white\", edgecolor=\"black\", boxstyle=\"round,pad=0.3\"))\n",
    "    axes[1,0].set_title(\"ROC Curve (RNF26 vs ZNF335)\")\n",
    "\n",
    "# 4. Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "    display_labels=['RNF26 (-1)','Neutral (0)','ZNF335 (+1)'])\n",
    "disp.plot(ax=axes[1,1], cmap=\"BuPu\", colorbar=False)\n",
    "axes[1,1].set_title(\"Confusion Matrix\")\n",
    "\n",
    "# 5. Loss Curve\n",
    "if hist is not None:\n",
    "    axes[2,0].plot(hist[\"epoch\"], hist[\"train_loss\"], label=\"Train Loss\", color=custom_colors[3])\n",
    "    axes[2,0].plot(hist[\"epoch\"], hist[\"val_loss\"], label=\"Val Loss\", color=custom_colors[0])\n",
    "    min_idx = hist[\"val_loss\"].idxmin()\n",
    "    axes[2,0].scatter(hist[\"epoch\"][min_idx], hist[\"val_loss\"][min_idx], color=\"black\")\n",
    "    axes[2,0].text(hist[\"epoch\"][min_idx], hist[\"val_loss\"][min_idx],\n",
    "                   f\"Min Val Loss={hist['val_loss'][min_idx]:.3f}\\n(Epoch {hist['epoch'][min_idx]})\",\n",
    "                   fontsize=9, va=\"bottom\", ha=\"left\",\n",
    "                   bbox=dict(facecolor=\"white\", edgecolor=\"black\", boxstyle=\"round,pad=0.3\"))\n",
    "    axes[2,0].set_title(\"Loss Curve\"); axes[2,0].legend()\n",
    "\n",
    "# 6. Accuracy Curve\n",
    "if hist is not None and \"val_acc\" in hist:\n",
    "    axes[2,1].plot(hist[\"epoch\"], hist[\"val_acc\"], label=\"Val Accuracy\", color=custom_colors[2])\n",
    "    max_idx = hist[\"val_acc\"].idxmax()\n",
    "    axes[2,1].scatter(hist[\"epoch\"][max_idx], hist[\"val_acc\"][max_idx], color=\"black\")\n",
    "    axes[2,1].text(hist[\"epoch\"][max_idx], hist[\"val_acc\"][max_idx],\n",
    "                   f\"Max Val Acc={hist['val_acc'][max_idx]:.3f}\\n(Epoch {hist['epoch'][max_idx]})\",\n",
    "                   fontsize=9, va=\"bottom\", ha=\"left\",\n",
    "                   bbox=dict(facecolor=\"white\", edgecolor=\"black\", boxstyle=\"round,pad=0.3\"))\n",
    "    axes[2,1].set_title(\"Validation Accuracy\"); axes[2,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"evaluation_training_summary_full.pdf\", dpi=300)\n",
    "plt.close(fig)\n",
    "\n",
    "# -----------------------------\n",
    "# Individual plots (regenerate fresh)\n",
    "# -----------------------------\n",
    "def save_single_plot(fname, plot_func):\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    plot_func(ax)\n",
    "    fig.savefig(fname, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "save_single_plot(\"scatter_only.pdf\", lambda ax: ax.scatter(\n",
    "    y_true, y_pred, c=[color_map[int(lbl)] for lbl in y_true],\n",
    "    alpha=0.6, edgecolor=\"k\"))\n",
    "\n",
    "save_single_plot(\"hist_only.pdf\", lambda ax: [\n",
    "    ax.hist(y_pred[y_true==cls], bins=20, alpha=0.8, label=label, color=custom_colors[i])\n",
    "    for i,(cls,label) in enumerate([(-1,\"RNF26\"),(0,\"Neutral\"),(1,\"ZNF335\")])\n",
    "])\n",
    "\n",
    "if fpr is not None:\n",
    "    save_single_plot(\"roc_only.pdf\", lambda ax: [\n",
    "        ax.plot(fpr, tpr, lw=2, color=custom_colors[3]),\n",
    "        ax.plot([0,1],[0,1],'--',color='grey'),\n",
    "        ax.text(0.05, 0.95, f\"AUC={roc_auc:.3f}\", transform=ax.transAxes,\n",
    "                va=\"top\", ha=\"left\",\n",
    "                bbox=dict(facecolor=\"white\", edgecolor=\"black\", boxstyle=\"round,pad=0.3\"))\n",
    "    ])\n",
    "\n",
    "# Corrected confusion matrix save\n",
    "save_single_plot(\"cm_only.pdf\", lambda ax: ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm,\n",
    "    display_labels=['RNF26 (-1)','Neutral (0)','ZNF335 (+1)']\n",
    ").plot(ax=ax, cmap=\"BuPu\", colorbar=False))\n",
    "\n",
    "if hist is not None:\n",
    "    save_single_plot(\"loss_only.pdf\", lambda ax: [\n",
    "        ax.plot(hist[\"epoch\"], hist[\"train_loss\"], label=\"Train\", color=custom_colors[3]),\n",
    "        ax.plot(hist[\"epoch\"], hist[\"val_loss\"], label=\"Val\", color=custom_colors[0]),\n",
    "        ax.legend()\n",
    "    ])\n",
    "    if \"val_acc\" in hist:\n",
    "        save_single_plot(\"acc_only.pdf\", lambda ax: [\n",
    "            ax.plot(hist[\"epoch\"], hist[\"val_acc\"], label=\"Val Acc\", color=custom_colors[2]),\n",
    "            ax.legend()\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15892d09-fc7e-4e5d-a15c-179eb22b80e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Inference (Regression: RNF=-1, Neutral=0, ZNF=+1) ====\n",
    "import os, re, cv2\n",
    "import numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import label, regionprops_table\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score, accuracy_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "test_dir    = \"New_image_directory\"\n",
    "output_xlsx = \"gene_similarity_predictions_inferenceFullList.xlsx\"\n",
    "names_csv   = \"names.csv\"\n",
    "model_path  = \"similarity_model_best.pt\"\n",
    "resize_size, half_crop, batch_size = 224, 50, 64\n",
    "model_channels = [\"GFP\"]\n",
    "\n",
    "#Update channel mapping as needed:\n",
    "channel_map = {\"DAPI\": \"_w1.TIF\",\"GFP\": \"_w2.TIF\",\"CellMask\": \"_w3.TIF\"}\n",
    "\n",
    "# -----------------------------\n",
    "# Gene mapping -- MAY NEED TO BE CUSTOMIZED\n",
    "# -----------------------------\n",
    "raw = pd.read_csv(names_csv, skiprows=2).iloc[:,[0,1,2]]\n",
    "raw.columns = [\"Gene\",\"Plate\",\"Well\"]\n",
    "raw[\"merge_key\"] = raw[\"Plate\"].astype(str).str.strip()+\"_\"+raw[\"Well\"].astype(str).str.strip()\n",
    "plate2gene = dict(zip(raw[\"merge_key\"], raw[\"Gene\"]))\n",
    "\n",
    "MERGE_KEY_REGEX = re.compile(r\"(AC\\d+_[A-Za-z]\\d+)\")\n",
    "def extract_merge_key(p): return MERGE_KEY_REGEX.search(p).group(1)\n",
    "\n",
    "# -----------------------------\n",
    "# Model (same as training)\n",
    "# -----------------------------\n",
    "class ResNetRegression(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, freeze_until_layer=6):\n",
    "        super().__init__()\n",
    "        base = resnet18(weights=\"IMAGENET1K_V1\")\n",
    "        child_counter = 0\n",
    "        for child in base.children():\n",
    "            child_counter += 1\n",
    "            if child_counter < freeze_until_layer:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "        num_ftrs = base.fc.in_features\n",
    "        base.fc = nn.Identity()\n",
    "        self.base = base\n",
    "        self.fc1 = nn.Linear(num_ftrs, hidden_dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, x, return_embed=False):\n",
    "        if x.shape[1] < 3:\n",
    "            pad = torch.zeros((x.shape[0], 3-x.shape[1], x.shape[2], x.shape[3]),\n",
    "                              device=x.device, dtype=x.dtype)\n",
    "            x = torch.cat([x, pad], dim=1)\n",
    "        feats = self.base(x)\n",
    "        if return_embed: return feats\n",
    "        x = self.fc1(feats); x = self.relu(x); x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=ResNetRegression(hidden_dim=64, freeze_until_layer=6).to(device)\n",
    "model.load_state_dict(torch.load(model_path,map_location=device))\n",
    "model.eval()\n",
    "print(\"Model loaded\")\n",
    "\n",
    "# -----------------------------\n",
    "# Storage\n",
    "# -----------------------------\n",
    "gene_preds=defaultdict(list)\n",
    "gene_embeds=defaultdict(list)\n",
    "\n",
    "# -----------------------------\n",
    "# Loop over images\n",
    "# -----------------------------\n",
    "file_list=sorted([f for f in os.listdir(test_dir) if f.endswith(channel_map[\"DAPI\"])])\n",
    "#file_list = file_list[::4]  # downsample for speed (same as old)\n",
    "print(f\"Processing {len(file_list)} images\")\n",
    "\n",
    "for fname in tqdm(file_list, desc=\"Scanning images\"):\n",
    "    prefix=fname.replace(channel_map[\"DAPI\"],\"\")\n",
    "    dapi=tiff.imread(os.path.join(test_dir,prefix+channel_map[\"DAPI\"]))\n",
    "    imgs={ch:tiff.imread(os.path.join(test_dir,prefix+channel_map[ch])) for ch in model_channels}\n",
    "    if any(img.shape!=dapi.shape for img in imgs.values()): continue\n",
    "    \n",
    "    thresh=threshold_otsu(dapi)\n",
    "    labeled=label(dapi>thresh)\n",
    "    props=regionprops_table(labeled,properties=(\"centroid\",))\n",
    "    \n",
    "    patches=[]\n",
    "    for cy,cx in zip(props[\"centroid-0\"],props[\"centroid-1\"]):\n",
    "        y1,y2,x1,x2=int(cy-half_crop),int(cy+half_crop),int(cx-half_crop),int(cx+half_crop)\n",
    "        if y1<0 or y2>dapi.shape[0] or x1<0 or x2>dapi.shape[1]: continue\n",
    "        chans=[cv2.resize(imgs[ch][y1:y2,x1:x2],(resize_size,resize_size)) for ch in model_channels]\n",
    "        patches.append(np.stack(chans,axis=0).astype(np.float32)/65535.0)\n",
    "    if not patches: continue\n",
    "    \n",
    "    patches=torch.tensor(np.stack(patches),dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        feats=model(patches,return_embed=True).cpu().numpy()\n",
    "        preds=model(patches).cpu().numpy().ravel()\n",
    "    \n",
    "    merge_key=extract_merge_key(prefix)\n",
    "    gene=plate2gene.get(merge_key,merge_key)\n",
    "    gene_embeds[gene].append(feats)\n",
    "    gene_preds[gene].extend(preds)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Aggregate by gene (with ≤500 cells cap)\n",
    "# -----------------------------\n",
    "max_cells = 500\n",
    "rng = np.random.default_rng(42)  # reproducible\n",
    "\n",
    "gene_centroids = {}\n",
    "gene_outputs = {}\n",
    "\n",
    "for gene in gene_preds:\n",
    "    preds = np.array(gene_preds[gene])\n",
    "    embeds = np.vstack(gene_embeds[gene])\n",
    "\n",
    "    # If more than max_cells, randomly sample indices\n",
    "    if len(preds) > max_cells:\n",
    "        idx = rng.choice(len(preds), size=max_cells, replace=False)\n",
    "        preds = preds[idx]\n",
    "        embeds = embeds[idx]\n",
    "\n",
    "    # Compute centroid and store\n",
    "    gene_centroids[gene] = embeds.mean(axis=0)\n",
    "    gene_outputs[gene] = preds\n",
    "\n",
    "\n",
    "# Anchor centroids\n",
    "znf = gene_centroids.get(\"ZNF335\")\n",
    "rnf = gene_centroids.get(\"RNF26\")\n",
    "b02 = gene_centroids.get(\"AC0559_B02\")  # <-- replace with correct merge_key/Gene name for B02 neutral anchor\n",
    "\n",
    "# -----------------------------\n",
    "# Results\n",
    "# -----------------------------\n",
    "results = []\n",
    "for gene, preds in gene_outputs.items():\n",
    "    cent = gene_centroids[gene]\n",
    "\n",
    "    # Similarities\n",
    "    sim_zn = cosine_similarity([cent], [znf])[0][0] if znf is not None else np.nan\n",
    "    sim_rn = cosine_similarity([cent], [rnf])[0][0] if rnf is not None else np.nan\n",
    "    sim_b0 = cosine_similarity([cent], [b02])[0][0] if b02 is not None else np.nan\n",
    "\n",
    "    # Relative scores (unchanged!)\n",
    "    rel = sim_zn - sim_rn if not (np.isnan(sim_zn) or np.isnan(sim_rn)) else np.nan\n",
    "    diff_zn_b0 = sim_zn - sim_b0 if not (np.isnan(sim_zn) or np.isnan(sim_b0)) else np.nan\n",
    "    diff_b0_rn = sim_b0 - sim_rn if not (np.isnan(sim_b0) or np.isnan(sim_rn)) else np.nan\n",
    "\n",
    "    results.append({\n",
    "        \"Gene\": gene,\n",
    "        \"sim_ZNF335\": sim_zn,\n",
    "        \"sim_B02\": sim_b0,\n",
    "        \"sim_RNF26\": sim_rn,\n",
    "        \"relative_score\": rel,\n",
    "        \"diff_ZNF335_B02\": diff_zn_b0,\n",
    "        \"diff_B02_RNF26\": diff_b0_rn,\n",
    "        \"n_cells\": len(preds)\n",
    "    })\n",
    "\n",
    "gene_df = pd.DataFrame(results).sort_values(\"relative_score\", ascending=False)\n",
    "gene_df.to_excel(output_xlsx, index=False)\n",
    "print(f\"Saved {output_xlsx}\")\n",
    "\n",
    "print(\"\\n Top 10 genes closest to ZNF335:\")\n",
    "print(gene_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n Top 10 genes closest to RNF26:\")\n",
    "print(gene_df.tail(10).to_string(index=False))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8530de-cac2-46a8-af85-4e1464e8a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Results\n",
    "# -----------------------------\n",
    "results = []\n",
    "for gene, preds in gene_outputs.items():\n",
    "    cent = gene_centroids[gene]\n",
    "\n",
    "    # Similarities\n",
    "    sim_zn = cosine_similarity([cent], [znf])[0][0] if znf is not None else np.nan\n",
    "    sim_rn = cosine_similarity([cent], [rnf])[0][0] if rnf is not None else np.nan\n",
    "    sim_b0 = cosine_similarity([cent], [b02])[0][0] if b02 is not None else np.nan\n",
    "\n",
    "    # Relative scores (unchanged!)\n",
    "    rel = sim_zn - sim_rn if not (np.isnan(sim_zn) or np.isnan(sim_rn)) else np.nan\n",
    "    diff_zn_b0 = sim_zn - sim_b0 if not (np.isnan(sim_zn) or np.isnan(sim_b0)) else np.nan\n",
    "    diff_b0_rn = sim_b0 - sim_rn if not (np.isnan(sim_b0) or np.isnan(sim_rn)) else np.nan\n",
    "\n",
    "    results.append({\n",
    "        \"Gene\": gene,\n",
    "        \"sim_ZNF335\": sim_zn,\n",
    "        \"sim_B02\": sim_b0,\n",
    "        \"sim_RNF26\": sim_rn,\n",
    "        \"relative_score\": rel,\n",
    "        \"diff_ZNF335_B02\": diff_zn_b0,\n",
    "        \"diff_B02_RNF26\": diff_b0_rn,\n",
    "        \"n_cells\": len(preds)\n",
    "    })\n",
    "\n",
    "gene_df = pd.DataFrame(results).sort_values(\"relative_score\", ascending=False)\n",
    "gene_df.to_excel(output_xlsx, index=False)\n",
    "print(f\"Saved {output_xlsx}\")\n",
    "\n",
    "print(\"\\n Top 10 genes closest to ZNF335:\")\n",
    "print(gene_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n Top 10 genes closest to RNF26:\")\n",
    "print(gene_df.tail(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade4a3b8-b5ea-48e0-b090-93989ca3dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load both files\n",
    "sim_df = pd.read_excel(\"gene_similarity_predictions_inferenceFullList.xlsx\")\n",
    "zscore_df = pd.read_excel(\"Zscore.xlsx\")\n",
    "\n",
    "# Standardize gene column\n",
    "sim_df.rename(columns={sim_df.columns[0]: \"Gene\"}, inplace=True)\n",
    "zscore_df.rename(columns={zscore_df.columns[0]: \"Gene\"}, inplace=True)\n",
    "\n",
    "# Merge in Z-Score\n",
    "merged = pd.merge(sim_df, zscore_df[[\"Gene\", \"Z-Score\"]], on=\"Gene\", how=\"left\")\n",
    "\n",
    "# Keep only desired columns\n",
    "keep_cols = [\n",
    "    \"Gene\", \"sim_ZNF335\", \"sim_B02\", \"sim_RNF26\",\n",
    "    \"relative_score\", \"diff_ZNF335_B02\", \"diff_B02_RNF26\",\n",
    "    \"n_cells\", \"Z-Score\"\n",
    "]\n",
    "merged = merged[keep_cols]\n",
    "\n",
    "# Keep only one row per Gene: highest Z-Score if present,\n",
    "# otherwise just the first available row\n",
    "merged = (\n",
    "    merged.sort_values(\"Z-Score\", ascending=False, na_position=\"last\")\n",
    "    .groupby(\"Gene\")\n",
    "    .head(1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Add normalization (0–100) and rank columns\n",
    "for col in [\"sim_ZNF335\", \"sim_B02\", \"sim_RNF26\"]:\n",
    "    # Linear normalization 0–100\n",
    "    merged[f\"{col}_norm100\"] = 100 * (merged[col] - merged[col].min()) / (merged[col].max() - merged[col].min())\n",
    "    \n",
    "    # Integer rank (1 = most similar, highest value first)\n",
    "    merged[f\"{col}_rank\"] = merged[col].rank(method=\"min\", ascending=False).astype(int)\n",
    "\n",
    "# Save\n",
    "out_file = \"final_similarity_with_ranks.xlsx\"\n",
    "merged.to_excel(out_file, index=False)\n",
    "\n",
    "print(f\"Saved {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24462c1e-0069-4fc2-bdca-5b44954d1665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "cluster_colors = [\"#651f56\", \"#e13c3d\", \"#f2704e\"]\n",
    "\n",
    "# ---- Load gene_df from your processed file ----\n",
    "gene_df = pd.read_excel(\"final_similarity_with_ranks.xlsx\")\n",
    "\n",
    "# ---- Build embedding ----\n",
    "genes, X = [], []\n",
    "for g, v in gene_centroids.items():\n",
    "    genes.append(g)\n",
    "    X.append(v)\n",
    "X = np.vstack(X)\n",
    "\n",
    "# ---- Dimensionality reduction ----\n",
    "try:\n",
    "    import umap\n",
    "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1,\n",
    "                        metric=\"cosine\", random_state=42)\n",
    "    Z = reducer.fit_transform(X)\n",
    "except Exception:\n",
    "    print(\"UMAP not available; falling back to PCA.\")\n",
    "    Z = PCA(n_components=3, random_state=42).fit_transform(X)\n",
    "\n",
    "# ---- Build df ----\n",
    "df_umap = pd.DataFrame({\"Gene\": genes, \"UMAP1\": Z[:,0], \"UMAP2\": Z[:,1]})\n",
    "df_umap = df_umap.merge(gene_df[[\"Gene\", \"Z-Score\"]], on=\"Gene\", how=\"left\")\n",
    "\n",
    "# ---- Mask Z-scores for B02 wells ----\n",
    "df_umap.loc[df_umap[\"Gene\"].str.endswith(\"B02\", na=False), \"Z-Score\"] = np.nan\n",
    "\n",
    "# ---- KMeans clustering ----\n",
    "kmeans = KMeans(n_clusters=3, random_state=42).fit(df_umap[[\"UMAP1\",\"UMAP2\"]].values)\n",
    "df_umap[\"Cluster\"] = kmeans.labels_\n",
    "\n",
    "# ---- Anchors ----\n",
    "anchors = [\"RNF26\", \"ZNF335\", \"AC0559_B02\"]\n",
    "anchor_colors = {\"RNF26\": \"orange\", \"ZNF335\": \"lightblue\", \"AC0559_B02\": \"green\"}\n",
    "\n",
    "# ---- Z-Score colormap ----\n",
    "zscore_colors = [\"#651f56\", \"#921d5c\", \"#e13c3d\", \"#f2704e\"]\n",
    "zscore_cmap = LinearSegmentedColormap.from_list(\"zscore_cmap\", zscore_colors, N=256)\n",
    "\n",
    "# ---- Scatter ----\n",
    "plt.figure(figsize=(10,7))\n",
    "sc = plt.scatter(\n",
    "    df_umap[\"UMAP1\"], df_umap[\"UMAP2\"],\n",
    "    c=df_umap[\"Z-Score\"], cmap=zscore_cmap,\n",
    "    s=30, alpha=1, vmin=2\n",
    ")\n",
    "\n",
    "# ---- Cluster boundaries ----\n",
    "for cluster_id in np.unique(df_umap[\"Cluster\"]):\n",
    "    points = df_umap.loc[df_umap[\"Cluster\"]==cluster_id, [\"UMAP1\",\"UMAP2\"]].values\n",
    "    if len(points) >= 3:\n",
    "        hull = ConvexHull(points)\n",
    "        plt.fill(points[hull.vertices,0], points[hull.vertices,1],\n",
    "                 facecolor=cluster_colors[cluster_id % len(cluster_colors)],\n",
    "                 edgecolor=cluster_colors[cluster_id % len(cluster_colors)],\n",
    "                 alpha=0.3, linewidth=2, label=f\"Cluster {cluster_id}\")\n",
    "\n",
    "# ---- Highlight anchors (stars only, no text) ----\n",
    "for _, row in df_umap[df_umap[\"Gene\"].isin(anchors)].iterrows():\n",
    "    g = row[\"Gene\"]\n",
    "    plt.scatter(row[\"UMAP1\"], row[\"UMAP2\"], s=200, marker=\"*\",\n",
    "                color=anchor_colors[g], edgecolor=\"black\", zorder=5)\n",
    "\n",
    "# ---- Final touches ----\n",
    "plt.colorbar(sc, label=\"Z-Score (NaN for B02 genes)\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"UMAP1\")\n",
    "plt.ylabel(\"UMAP2\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save as PDF\n",
    "plt.savefig(\"umap_gene_embeddings_Zscore_clusters.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7acdfd4-8847-44ac-8979-2d000773f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save relationships of each gene relative to their cluster and distance from anchor point (RNF26/ZNF335/NT Ctrl)\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# ---- Map each anchor to its cluster ----\n",
    "anchor_clusters = {}\n",
    "for anchor in anchors:\n",
    "    if anchor in df_umap[\"Gene\"].values:\n",
    "        anchor_cluster = df_umap.loc[df_umap[\"Gene\"]==anchor, \"Cluster\"].iloc[0]\n",
    "        anchor_clusters[anchor] = anchor_cluster\n",
    "\n",
    "# ---- Compute distance to anchor of the same cluster ----\n",
    "distances = []\n",
    "for idx, row in df_umap.iterrows():\n",
    "    gene = row[\"Gene\"]\n",
    "    cluster = row[\"Cluster\"]\n",
    "    \n",
    "    # Find anchor for this cluster\n",
    "    anchor_for_cluster = None\n",
    "    for a, c in anchor_clusters.items():\n",
    "        if c == cluster:\n",
    "            anchor_for_cluster = a\n",
    "            break\n",
    "    \n",
    "    if anchor_for_cluster is not None:\n",
    "        # Compute Euclidean distance in UMAP space\n",
    "        anchor_coords = df_umap.loc[df_umap[\"Gene\"]==anchor_for_cluster, [\"UMAP1\",\"UMAP2\"]].values[0]\n",
    "        gene_coords = row[[\"UMAP1\",\"UMAP2\"]].values\n",
    "        dist = np.linalg.norm(gene_coords - anchor_coords)\n",
    "    else:\n",
    "        dist = np.nan\n",
    "    \n",
    "    distances.append(dist)\n",
    "\n",
    "df_umap[\"Dist_to_Anchor\"] = distances\n",
    "\n",
    "# ---- Save results ----\n",
    "df_umap.to_excel(\"gene_cluster_anchor_distances.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec3c23-8698-4047-b9b1-b7c3eb456265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap plotting/clustering\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Set global font to Arial\n",
    "mpl.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "g = sns.clustermap(\n",
    "    rank_df,\n",
    "    cmap=custom_cmap,\n",
    "    metric=\"euclidean\",\n",
    "    method=\"ward\",\n",
    "    col_cluster=False,\n",
    "    figsize=(7, 18),\n",
    "    cbar_kws={\"label\": \"Rank\"},\n",
    "    dendrogram_ratio=(0.35, 0.05),\n",
    "    colors_ratio=0.01,\n",
    "    tree_kws={\"linewidths\": 0.8}\n",
    ")\n",
    "\n",
    "# Row labels\n",
    "g.ax_heatmap.set_yticks(range(len(rank_df)))\n",
    "g.ax_heatmap.set_yticklabels(\n",
    "    g.data2d.index,\n",
    "    fontsize=12,\n",
    "    rotation=0\n",
    ")\n",
    "\n",
    "# Column labels\n",
    "g.ax_heatmap.set_xticklabels(\n",
    "    g.data2d.columns,\n",
    "    fontsize=20,\n",
    "    ha=\"right\"\n",
    ")\n",
    "\n",
    "# Colorbar font\n",
    "g.cax.yaxis.label.set_size(12)\n",
    "g.cax.tick_params(labelsize=10)\n",
    "g.savefig(\"clustermap2.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# Or if you want to also see it interactively:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcada3e-6f2d-4d44-96e5-c740cc54876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For elbow method plot:\n",
    "#OPTIONAL\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "# ----------------------------------\n",
    "# 1. Elbow Plot\n",
    "# ----------------------------------\n",
    "with PdfPages(\"combined_panels.pdf\") as pdf:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(K, inertias, \"o-\", lw=2, color=\"#651f50\")\n",
    "    plt.xlabel(\"Number of clusters (k)\")\n",
    "    plt.ylabel(\"Within-cluster sum of squares (Inertia)\")\n",
    "    plt.title(\"Elbow Method for Optimal k\")\n",
    "    plt.xticks(K)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    pdf.savefig()\n",
    "    plt.close()\n",
    "\n",
    "    # ----------------------------------\n",
    "    # 2. Silhouette plot (k=3 only)\n",
    "    # ----------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42).fit(X_umap)\n",
    "    labels = kmeans.labels_\n",
    "    sil_vals = silhouette_samples(X_umap, labels)\n",
    "    sil_avg = silhouette_score(X_umap, labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    cluster_colors = [\"#ff6c20\", \"#e13c3d\", \"#651f50\"]  # 3 cluster colors\n",
    "    for i in range(3):\n",
    "        ith_vals = sil_vals[labels == i]\n",
    "        ith_vals.sort()\n",
    "        size = len(ith_vals)\n",
    "        y_upper = y_lower + size\n",
    "        color = cluster_colors[i % len(cluster_colors)]\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_vals,\n",
    "                         facecolor=color, alpha=0.8)\n",
    "        ax.text(-0.05, y_lower + 0.5 * size, str(i))\n",
    "        y_lower = y_upper + 10\n",
    "    ax.axvline(x=sil_avg, color=\"#651f50\", linestyle=\"--\", lw=2)\n",
    "    ax.set_title(f\"Silhouette plot for k=3 (avg={sil_avg:.3f})\")\n",
    "    ax.set_xlabel(\"Silhouette coefficient\")\n",
    "    ax.set_ylabel(\"Cluster\")\n",
    "    pdf.savefig()\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
